{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\r\n",
        "from azureml.core import Workspace, Datastore, Dataset\r\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.runconfig import RunConfiguration\r\n",
        "from azureml.core.conda_dependencies import CondaDependencies\r\n",
        "from azureml.core import Environment \r\n",
        "from azureml.data import OutputFileDatasetConfig\r\n",
        "\r\n",
        "ws = Workspace.from_config()\r\n",
        "\r\n",
        "compute_name = \"avisekCompute\"\r\n",
        "#compute_name = \"avisek-computeCluster\"\r\n",
        "compute_target = ws.compute_targets[compute_name]\r\n",
        "\r\n",
        "\r\n",
        "# commandstep_run_config = RunConfiguration(compute_target)\r\n",
        "# commandstep_run_config.environment = Environment.get(ws,name='commandstep-env')\r\n",
        "env = Environment.get(ws,name='commandstepR-env')\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1652909091252
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\r\n",
        "\r\n",
        "datastore = ws.get_default_datastore()\r\n",
        "pg_dataset = Dataset.File.from_files(datastore.path('penguin_data'))\r\n",
        "pg_dataset"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1652909098490
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import ScriptRunConfig\r\n",
        "from azureml.pipeline.core import PipelineData\r\n",
        "\r\n",
        "penguin_data = PipelineData(\"penguin_data\", datastore=datastore)\r\n",
        "#validated_data = PipelineData(\"validated_data\", datastore=datastore)\r\n",
        "validated_data = OutputFileDatasetConfig(name=\"validated_data\", destination=(datastore, \"validated_data\")).as_upload(overwrite=True)\r\n",
        "train_data = OutputFileDatasetConfig(name=\"train_data\", destination=(datastore, \"train_data\")).as_upload(overwrite=True)\r\n",
        "test_data = OutputFileDatasetConfig(name=\"test_data\", destination=(datastore, \"test_data\")).as_upload(overwrite=True)\r\n",
        "model = OutputFileDatasetConfig(name=\"model\", destination=(datastore, \"model\")).as_upload(overwrite=True)\r\n",
        "\r\n",
        "src_dir = './'\r\n",
        "\r\n",
        "process_data = ScriptRunConfig(source_directory=src_dir,\r\n",
        "                            command=['Rscript process_data.R --penguin_data', pg_dataset.as_named_input(name=\"penguin_data\").as_mount(), '--output_folder', validated_data],\r\n",
        "                            compute_target=compute_target,\r\n",
        "                            environment=env)\r\n",
        "\r\n",
        "prepare_data = ScriptRunConfig(source_directory=src_dir,\r\n",
        "                            command=['Rscript prepare_data.R --validated_data', validated_data, '--train_folder', train_data, '--test_folder', test_data],\r\n",
        "                            compute_target=compute_target,\r\n",
        "                            environment=env)\r\n",
        "\r\n",
        "train = ScriptRunConfig(source_directory=src_dir,\r\n",
        "                            command=['Rscript train_model_dt.R --train_data', train_data, '--model_folder', model],\r\n",
        "                            compute_target=compute_target,\r\n",
        "                            environment=env)\r\n",
        "\r\n",
        "test = ScriptRunConfig(source_directory=src_dir,\r\n",
        "                            command=['Rscript test_model_dt.R --test_data', test_data, '--model_folder', model],\r\n",
        "                            compute_target=compute_target,\r\n",
        "                            environment=env)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1652893212968
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#step1_output_ds = validated_data.register_on_complete(name='validated_data', description = 'files from step1')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1652893213046
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.steps import CommandStep\r\n",
        "\r\n",
        "process_data_step = CommandStep(name='process_data', \r\n",
        "                    outputs = [validated_data],\r\n",
        "                    runconfig=process_data)\r\n",
        "\r\n",
        "prepare_data_step = CommandStep(name='prepare_data', \r\n",
        "                    inputs = [validated_data],\r\n",
        "                    outputs = [train_data, test_data],\r\n",
        "                    runconfig=prepare_data)\r\n",
        "\r\n",
        "train_step = CommandStep(name='model_training', \r\n",
        "                    inputs = [train_data],\r\n",
        "                    outputs = [model],\r\n",
        "                    runconfig=train)\r\n",
        "\r\n",
        "test_step = CommandStep(name='model_scoring', \r\n",
        "                    inputs = [test_data, model],\r\n",
        "                    #outputs = [model],\r\n",
        "                    runconfig=test)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1652893213137
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of steps to run (`compare_step` definition not shown)\r\n",
        "poc_pipeline_R = [test_step]\r\n",
        "\r\n",
        "from azureml.pipeline.core import Pipeline\r\n",
        "\r\n",
        "# Build the pipeline\r\n",
        "pipeline1 = Pipeline(workspace=ws, steps=[poc_pipeline_R])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "from azureml.core import Experiment\r\n",
        "\r\n",
        "# Submit the pipeline to be run\r\n",
        "pipeline_run1 = Experiment(ws, 'POC_PENGUIN_DATA_CMDSTEP').submit(pipeline1)\r\n",
        "pipeline_run1.wait_for_completion()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1652893494631
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.model import Model\r\n",
        "import os\r\n",
        "\r\n",
        "datastore.download(os.getcwd(), prefix='model/model_dt.rds', overwrite = True)\r\n",
        "\r\n",
        "#Get the working Dir\r\n",
        "wkDir  = os.getcwd()\r\n",
        "# Name of the create Directory\r\n",
        "dataDir = \"model/model_dt.rds\"  \r\n",
        "# Path\r\n",
        "path = os.path.join(wkDir, dataDir)\r\n",
        "\r\n",
        "myModel = Model.register(model_path=path,\r\n",
        "                          model_name=\"decision_tree_model\",\r\n",
        "                          tags={'area': \"penguin data\", 'type': \"classification\"},\r\n",
        "                          description=\"Decision Tree model to predict Penguin Species\",\r\n",
        "                          workspace=ws)\r\n",
        "\r\n",
        "print('Name:', myModel.name)\r\n",
        "print('Version:', myModel.version)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1652909311669
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}